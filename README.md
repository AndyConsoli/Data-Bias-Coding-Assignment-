# Data-Bias-Coding-Assignment-
API Key: 'AIzaSyAZTpJeKTNovPTRfFJV2n_PrX-5u64uLMg' 
I will be testing if there is data bias in informal/formal pieces of context compared with anti-male/anti-female data within the data set that was given.
Hypothesis: I believe that the informal pieces of content will bring the result of more toxic outcomes. This hypothesis is based on my own personal experience with toxicity of content on the internet where longer/formal types of content are typically less offensive than informal pieces of content like tweets on Twitter or comments on Instagram. I am predicting that the scores of informal content will be above .5 in the toxicity range while formal pieces will be under the .5 threshold which would make these results as non toxic. I will also be checking anti-male and anti-female comments on informal/formal content and I predict that anti-female comments will result in more toxic outcomes than anti-male content.
Results: Based on the test I ran there was enough evidence for me to see that there is a slight bias in the data for females than males. The anti-female scores were slightly more toxic than the anti-male scores. I also was able to gather that the ML algorithm is biased towards word choice in the code. For example, saying cuss words in the code rather than swear words made the results more toxic by at least .2 points. When putting actual curse words in the algorithm it would present toxic results, but by my code being more generalized and saying cuss words to get the full picture the results were non-toxic. 
My results were that the data is not toxic when compared to informal and formal data. The same case was presented with anti-female and anti-male bias. The highest score was when the data searched for informal content and anti-women, the score was .3625127. These results mean my hypothesis is wrong, I predicted that the informal context as well as the anti-female context would result in a toxic outcome but none of my scores were over .4. 
	  I think the results are the way they are because the program was designed to exclude bias in the code. I believe this because of the results I got when comparing anti-male and anti-female outcomes. The outcomes were very similar to each other with the average difference of the scores being .0296784 (this score is formed by subtracting the anti-male score from the anti-female score on both informal and formal contexts and adding both scores from the informal and formal data and dividing it by two). These outcomes provided me with enough information to show that there is no biased data in the dataset for the gender category. With this in mind, a theory that makes the most sense for me is that the algorithm was designed to not take gender into account in results.  The same theory can be implemented to the informal and formal content because the scores were very similar to each other (less than .1 off of each other) which makes me believe that there is no bias between these two categories of information. 
